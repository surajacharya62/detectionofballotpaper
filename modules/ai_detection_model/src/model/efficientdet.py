# -*- coding: utf-8 -*-
"""EfficientDet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sQ4eO15rGmiyn5Qy2HUxq-wHrNx2JewV
"""

from google.colab import drive
drive.mount('/content/drive')

cd drive/MyDrive/modules/

ls

import pandas as pd
import numpy as np
from PIL import Image
import matplotlib.patches as patches
import matplotlib.pyplot as plt
import os
import ast

import numpy as np
from sklearn.metrics import accuracy_score
import warnings
warnings.filterwarnings("ignore")

from torch.utils.data import Dataset
import torchvision.transforms as T
import torch
import torchvision
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torch.utils.data import DataLoader
from torchvision.transforms.functional import to_pil_image
from torch.cuda.amp import GradScaler, autocast

pip install pytorch-lightning

!pip install albumentations==1.0.0
!pip install effdet
!pip install ensemble-boxes==1.0.6
!pip install fastcore==1.3.20
# !pip install pytorch-lightning==1.3.5
!pip install timm==0.4.9
!pip install torch

from efficientdet.effdet.config.model_config import efficientdet_model_param_dict
from efficientdet.effdet import get_efficientdet_config, EfficientDet, DetBenchTrain
from efficientdet.effdet.efficientdet import HeadNet
from efficientdet.effdet.config.model_config import efficientdet_model_param_dict

print(f'number of configs: {len(efficientdet_model_param_dict)}')

list(efficientdet_model_param_dict.keys())[::3]

import timm
timm.list_models('tf_efficientnetv2_*')





import matplotlib.pyplot as plt
from matplotlib import patches

def get_rectangle_edges_from_pascal_bbox(bbox):
    xmin_top_left, ymin_top_left, xmax_bottom_right, ymax_bottom_right = bbox

    bottom_left = (xmin_top_left, ymax_bottom_right)
    width = xmax_bottom_right - xmin_top_left
    height = ymin_top_left - ymax_bottom_right

    return bottom_left, width, height

def draw_pascal_voc_bboxes(
    plot_ax,
    bboxes,
    get_rectangle_corners_fn=get_rectangle_edges_from_pascal_bbox,
):
    for bbox in bboxes:
        bottom_left, width, height = get_rectangle_corners_fn(bbox)

        rect_1 = patches.Rectangle(
            bottom_left,
            width,
            height,
            linewidth=4,
            edgecolor="red",
            fill=False,
        )
        rect_2 = patches.Rectangle(
            bottom_left,
            width,
            height,
            linewidth=2,
            edgecolor="white",
            fill=False,
        )

        # Add the patch to the Axes
        plot_ax.add_patch(rect_1)
        plot_ax.add_patch(rect_2)

def show_image(
    image, bboxes=None, draw_bboxes_fn=draw_pascal_voc_bboxes, figsize=(10, 10)
):
    fig, ax = plt.subplots(1, figsize=figsize)
    ax.imshow(image)

    if bboxes is not None:
        draw_bboxes_fn(ax, bboxes)

    plt.show()

from pathlib import Path

import PIL

import numpy as np

class ElectoralDatasetAdaptor:
    def __init__(self, images_dir_path, annotations_dataframe,labelid):
        self.images_dir_path = Path(images_dir_path)
        self.annotations_df = annotations_dataframe
        self.label = labelid
        self.images = self.annotations_df.image_id.unique().tolist()

    def __len__(self) -> int:
        return len(self.images)

    def get_image_and_labels_by_idx(self, index):
        image_name = self.images[index]
        image = PIL.Image.open(self.images_dir_path / image_name)
        pascal_bboxes = self.annotations_df[self.annotations_df.image_id == image_name][
            ["x_min", "y_min", "x_max", "y_max"]
        ].values

        filtered_rows = self.annotations_df[self.annotations_df['image_id'] == image_name]
        # # boxes = filtered_rows[['x1', 'y1', 'x2', 'y2']].values.astype('float32')
        labels = filtered_rows['label'].apply(lambda x: self.label[x]).values


        # labels = self.annotations_df[self.annotations_df.image_id == image_name]['label'].apply(lambda x: self.label[x]).values
        class_labels = labels

        return image, pascal_bboxes, class_labels, index

    def show_image(self, index):
        image, bboxes, class_labels, image_id = self.get_image_and_labels_by_idx(index)
        print(f"image_id: {image_id}")
        show_image(image, bboxes.tolist())
        print(class_labels)

df = pd.read_csv(os.path.join('training_set/set4/', 'annotationeff.csv'))
label_to_id = {label: i for i, label in enumerate(df['label'].unique())}
# print(label_to_id)
# Combine all unique labels from both training and test sets, then sort them
all_labels = sorted(label_to_id)

# Create a consistent mapping from label names to label IDs, starting from 1
label_to_id1 = {label: i for i, label in enumerate(all_labels)}

# Manually add the background label with ID 0
# label_to_id['stamp'] = 43

# Example of label_to_id mapping
print(label_to_id1)
df_train = pd.read_csv(os.path.join('training_set/set4/', 'annotationeff.csv'))
df_val = pd.read_csv(os.path.join('validation_set/set2/', 'annotationseff.csv'))

train_data_path = 'training_set/set4/train'
val_data_path = 'validation_set/set2/val'


electoral_train_ds = ElectoralDatasetAdaptor(train_data_path, df_train,label_to_id1)
electoral_val_ds = ElectoralDatasetAdaptor(val_data_path, df_val,label_to_id1)

electoral_train_ds.show_image(0)

electoral_val_ds.show_image(0)

def create_model(num_classes=43, image_size=2048, architecture="tf_efficientnetv2_l"):
    efficientdet_model_param_dict['tf_efficientnetv2_l'] = dict(
        name='tf_efficientnetv2_l',
        backbone_name='tf_efficientnetv2_l',
        backbone_args=dict(drop_path_rate=0.2),
        num_classes=num_classes,
        url='', )

    config = get_efficientdet_config(architecture)
    config.update({'num_classes': num_classes})
    config.update({'image_size': (image_size, image_size)})

    print(config)

    net = EfficientDet(config, pretrained_backbone=True)
    net.class_net = HeadNet(
        config,
        num_outputs=config.num_classes,
    )
    return DetBenchTrain(net, config)

num_classes = 43
img_size = 2048
model_architecture = 'tf_efficientnetv2_l'


device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model = create_model(
            num_classes, img_size, architecture=model_architecture
        )
optimizer = torch.optim.SGD(model.parameters(),lr=0.001, momentum=0.9, weight_decay=0.0005)
num_epochs = 15

from torch.utils.data import Dataset

import albumentations as A
from albumentations.pytorch.transforms import ToTensorV2

def get_train_transforms(target_img_size=2048):
    return A.Compose(
        [
            A.HorizontalFlip(p=0.5),
            A.Resize(height=target_img_size, width=target_img_size, p=1),
            A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
            ToTensorV2(p=1),
        ],
        p=1.0,
        bbox_params=A.BboxParams(
            format="pascal_voc", min_area=0, min_visibility=0, label_fields=["labels"]
        ),
    )


def get_valid_transforms(target_img_size=2048):
    return A.Compose(
        [
            A.Resize(height=target_img_size, width=target_img_size, p=1),
            A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
            ToTensorV2(p=1),
        ],
        p=1.0,
        bbox_params=A.BboxParams(
            format="pascal_voc", min_area=0, min_visibility=0, label_fields=["labels"]
        ),
    )

class EfficientDetDataset(Dataset):
    def __init__(
        self, dataset_adaptor, transforms=get_valid_transforms()
    ):
        self.ds = dataset_adaptor
        self.transforms = transforms

    def __getitem__(self, index):
        (
            image,
            pascal_bboxes,
            class_labels,
            image_id,
        ) = self.ds.get_image_and_labels_by_idx(index)

        sample = {
            "image": np.array(image, dtype=np.float32),
            "bboxes": pascal_bboxes,
            "labels": class_labels,
        }

        sample = self.transforms(**sample)
        sample["bboxes"] = np.array(sample["bboxes"])
        image = sample["image"]
        pascal_bboxes = sample["bboxes"]
        labels = sample["labels"]

        _, new_h, new_w = image.shape
        sample["bboxes"][:, [0, 1, 2, 3]] = sample["bboxes"][
            :, [1, 0, 3, 2]
        ]  # convert to yxyx

        target = {
            "bboxes": torch.as_tensor(sample["bboxes"], dtype=torch.float32),
            "labels": torch.as_tensor(labels),
            "image_id": torch.tensor([image_id]),
            "img_size": (new_h, new_w),
            "img_scale": torch.tensor([1.0]),
        }

        return image, target, image_id

    def __len__(self):
        return len(self.ds)

from pytorch_lightning import LightningDataModule
from torch.utils.data import DataLoader

class EfficientDetDataModule(LightningDataModule):

    def __init__(self,
                train_dataset_adaptor,
                validation_dataset_adaptor,
                train_transforms=get_train_transforms(target_img_size=2048),
                valid_transforms=get_valid_transforms(target_img_size=2048),
                num_workers=4,
                batch_size=4):

        self.train_ds = train_dataset_adaptor
        self.valid_ds = validation_dataset_adaptor
        self.train_tfms = train_transforms
        self.valid_tfms = valid_transforms
        self.num_workers = num_workers
        self.batch_size = batch_size
        super().__init__()

    def train_dataset(self) -> EfficientDetDataset:
        return EfficientDetDataset(
            dataset_adaptor=self.train_ds, transforms=self.train_tfms
        )

    def train_dataloader(self) -> DataLoader:
        train_dataset = self.train_dataset()
        train_loader = torch.utils.data.DataLoader(
            train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            pin_memory=True,
            drop_last=True,
            num_workers=self.num_workers,
            collate_fn=self.collate_fn,
        )

        return train_loader

    def val_dataset(self) -> EfficientDetDataset:
        return EfficientDetDataset(
            dataset_adaptor=self.valid_ds, transforms=self.valid_tfms
        )

    def val_dataloader(self) -> DataLoader:
        valid_dataset = self.val_dataset()
        valid_loader = torch.utils.data.DataLoader(
            valid_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            pin_memory=True,
            drop_last=True,
            num_workers=self.num_workers,
            collate_fn=self.collate_fn,
        )

        return valid_loader

    @staticmethod
    def collate_fn(batch):
        images, targets, image_ids = tuple(zip(*batch))
        images = torch.stack(images)
        images = images.float()

        boxes = [target["bboxes"].float() for target in targets]
        labels = [target["labels"].float() for target in targets]
        img_size = torch.tensor([target["img_size"] for target in targets]).float()
        img_scale = torch.tensor([target["img_scale"] for target in targets]).float()

        annotations = {
            "bbox": boxes,
            "cls": labels,
            "img_size": img_size,
            "img_scale": img_scale,
        }

        return images, annotations, targets, image_ids



# def collate_fn(batch):
#     images, targets, image_ids = tuple(zip(*batch))
#     images = torch.stack(images)
#     images = images.float()

#     # boxes = [target["bboxes"].float() for target in targets]
#     # labels = [target["labels"].float() for target in targets]
#     # img_size = torch.tensor([target["img_size"] for target in targets]).float()
#     # img_scale = torch.tensor([target["img_scale"] for target in targets]).float()

#     # annotations = {
#     #     "bbox": boxes,
#     #     "cls": labels,
#     #     "img_size": img_size,
#     #     "img_scale": img_scale,
#     # }

#     return images,  targets

# image_path = 'training_set/set3/'
# train_path = 'train'
# image_path_val = 'validation_set/set1/'
# val_path = 'val'

# # image_path_test = 'testing_set/set3/test'

# train_set = ElectoralSymbolDataset(image_path, train_path,label_to_id1, use_transforms=get_train_transforms(target_img_size=2048))

# train_loader = DataLoader(train_set, batch_size=4,
#                                          shuffle= True,
#                                          pin_memory= True if torch.cuda.is_available() else False, collate_fn=collate_fn )


# # test_set = UnlabeledTestDataset(image_path_test, transform=transform_image(True))
# # test_loader = DataLoader(test_set, batch_size=4, shuffle=False)

# val_set = ElectoralSymbolDataset(image_path_val,val_path,label_to_id1, use_transforms=get_valid_transforms(target_img_size=2048))
# val_loader = DataLoader(val_set, batch_size=4, shuffle=False,pin_memory= True if torch.cuda.is_available() else False, collate_fn=collate_fn)



# train_set[0]

# model.to(device)
# total_train_losses = []
# total_val_losses = []
# for epoch in range(num_epochs):
#     model.train()  # Set the model to training mode
#     epoch_train_loss = 0
#     scaler = GradScaler()
#     for images, targets in train_loader:
#         images = [image.to(device) for image in images]
#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

#         optimizer.zero_grad()
#         with autocast():
#             outputs = model(images, targets)
#             losses = sum(loss for loss in outputs.values())
#         # Backpropagation
#         scaler.scale(losses).backward()
#         scaler.step(optimizer)
#         scaler.update()

#         epoch_train_loss += losses.item()

#      # Average train loss for the epoch
#     epoch_train_loss /= len(train_set)
#     total_train_losses.append(epoch_train_loss)

#     #validation loss
#     val_loss = validation_loss(model, test_loader)
#     total_val_losses.append(val_loss)
#     print(f"Epoch: {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.3f}, Val Loss: {val_loss:.3f}")

#     torch.cuda.empty_cache()
#     torch.save(model.state_dict(), 'trained_model_efficientdet.pth')

from numbers import Number
from typing import List
from functools import singledispatch

import numpy as np
import torch
# import lightning L
from fastcore.dispatch import typedispatch
from pytorch_lightning import LightningModule
# from pytorch_lightning.core.decorators import auto_move_data


from ensemble_boxes import ensemble_boxes_wbf


def run_wbf(predictions, image_size=2048, iou_thr=0.44, skip_box_thr=0.43, weights=None):
    bboxes = []
    confidences = []
    class_labels = []

    for prediction in predictions:
        boxes = [(prediction["boxes"] / image_size).tolist()]
        scores = [prediction["scores"].tolist()]
        labels = [prediction["classes"].tolist()]

        boxes, scores, labels = ensemble_boxes_wbf.weighted_boxes_fusion(
            boxes,
            scores,
            labels,
            weights=weights,
            iou_thr=iou_thr,
            skip_box_thr=skip_box_thr,
        )
        boxes = boxes * (image_size - 1)
        bboxes.append(boxes.tolist())
        confidences.append(scores.tolist())
        class_labels.append(labels.tolist())

    return bboxes, confidences, class_labels


class EfficientDetModel(LightningModule):
    def __init__(
        self,
        num_classes=43,
        img_size=512,
        prediction_confidence_threshold=0.2,
        learning_rate=0.0002,
        wbf_iou_threshold=0.44,
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu"),
        inference_transforms=get_valid_transforms(target_img_size=512),
        model_architecture='tf_efficientnetv2_l',
    ):
        super().__init__()
        self.img_size = img_size
        self.model = create_model(
            num_classes, img_size, architecture=model_architecture
        )
        self.prediction_confidence_threshold = prediction_confidence_threshold
        self.lr = learning_rate
        self.wbf_iou_threshold = wbf_iou_threshold
        self.inference_tfms = inference_transforms



    def forward(self, images, targets):
        images = images.to(self.device)
        targets = {k: v.to(self.device) for k, v in targets.items()}
        return self.model(images, targets)

    def configure_optimizers(self):
        return torch.optim.AdamW(self.model.parameters(), lr=self.lr)


    def training_step(self, batch, batch_idx):
        train_loss = 0
        images, annotations, _, image_ids = batch
        print("Batch Len" + str(len(batch)))
        losses = self.model(images, annotations)
        # print(losses)
        # losses = sum(loss for loss in losses.values())
        # train_loss += losses.item()
        # print(f'Val Loss: {train_loss /len(batch)}')
        logging_losses = {
            "class_loss": losses["class_loss"].detach(),
            "box_loss": losses["box_loss"].detach(),
        }

        self.log("train_loss", losses["loss"], on_step=True, on_epoch=True, prog_bar=True,
                 logger=True)
        self.log(
            "train_class_loss", losses["class_loss"], on_step=True, on_epoch=True, prog_bar=True,
            logger=True
        )
        self.log("train_box_loss", losses["box_loss"], on_step=True, on_epoch=True, prog_bar=True,
                 logger=True)
        # print(losses['loss'])
        return losses['loss']


    @torch.no_grad()
    def validation_step(self, batch, batch_idx):
        val_loss = 0
        print("Batch Len" + str(len(batch)))
        images, annotations, targets, image_ids = batch
        outputs = self.model(images, annotations)
        # losses = sum(loss for loss in outputs.values())
        # val_loss += losses.item()
        # print(f'Val loss {val_loss/len(batch)}')
        # print(outputs)

        detections = outputs["detections"]

        batch_predictions = {
            "predictions": detections,
            "targets": targets,
            "image_ids": image_ids,
        }

        logging_losses = {
            "class_loss": outputs["class_loss"].detach(),
            "box_loss": outputs["box_loss"].detach(),
        }

        self.log("valid_loss", outputs["loss"], on_step=True, on_epoch=True, prog_bar=True,
                 logger=True, sync_dist=True)
        self.log(
            "valid_class_loss", logging_losses["class_loss"], on_step=True, on_epoch=True,
            prog_bar=True, logger=True, sync_dist=True
        )
        self.log("valid_box_loss", logging_losses["box_loss"], on_step=True, on_epoch=True,
                 prog_bar=True, logger=True, sync_dist=True)

        # print(outputs['loss'])
        return {'loss': outputs["loss"], 'batch_predictions': batch_predictions}


    @typedispatch
    def predict(self, images: List):
        """
        For making predictions from images
        Args:
            images: a list of PIL images

        Returns: a tuple of lists containing bboxes, predicted_class_labels, predicted_class_confidences

        """
        image_sizes = [(image.size[1], image.size[0]) for image in images]
        images_tensor = torch.stack(
            [
                self.inference_tfms(
                    image=np.array(image, dtype=np.float32),
                    labels=np.ones(1),
                    bboxes=np.array([[0, 0, 1, 1]]),
                )["image"]
                for image in images
            ]
        )

        return self._run_inference(images_tensor, image_sizes)

    @typedispatch
    def predict(self, images_tensor: torch.Tensor):
        """
        For making predictions from tensors returned from the model's dataloader
        Args:
            images_tensor: the images tensor returned from the dataloader

        Returns: a tuple of lists containing bboxes, predicted_class_labels, predicted_class_confidences

        """
        if images_tensor.ndim == 3:
            images_tensor = images_tensor.unsqueeze(0)
        if (
            images_tensor.shape[-1] != self.img_size
            or images_tensor.shape[-2] != self.img_size
        ):
            raise ValueError(
                f"Input tensors must be of shape (N, 3, {self.img_size}, {self.img_size})"
            )

        num_images = images_tensor.shape[0]
        image_sizes = [(self.img_size, self.img_size)] * num_images

        return self._run_inference(images_tensor, image_sizes)

    def _run_inference(self, images_tensor, image_sizes):
        dummy_targets = self._create_dummy_inference_targets(
            num_images=images_tensor.shape[0]
        )

        detections = self.model(images_tensor.to(self.device), dummy_targets)[
            "detections"
        ]
        (
            predicted_bboxes,
            predicted_class_confidences,
            predicted_class_labels,
        ) = self.post_process_detections(detections)

        scaled_bboxes = self.__rescale_bboxes(
            predicted_bboxes=predicted_bboxes, image_sizes=image_sizes
        )

        return scaled_bboxes, predicted_class_labels, predicted_class_confidences

    def _create_dummy_inference_targets(self, num_images):
        dummy_targets = {
            "bbox": [
                torch.tensor([[0.0, 0.0, 0.0, 0.0]], device=self.device)
                for i in range(num_images)
            ],
            "cls": [torch.tensor([1.0], device=self.device) for i in range(num_images)],
            "img_size": torch.tensor(
                [(self.img_size, self.img_size)] * num_images, device=self.device
            ).float(),
            "img_scale": torch.ones(num_images, device=self.device).float(),
        }

        return dummy_targets

    def post_process_detections(self, detections):
        predictions = []
        for i in range(detections.shape[0]):
            predictions.append(
                self._postprocess_single_prediction_detections(detections[i])
            )

        predicted_bboxes, predicted_class_confidences, predicted_class_labels = run_wbf(
            predictions, image_size=self.img_size, iou_thr=self.wbf_iou_threshold
        )

        return predicted_bboxes, predicted_class_confidences, predicted_class_labels

    def _postprocess_single_prediction_detections(self, detections):
        boxes = detections.detach().cpu().numpy()[:, :4]
        scores = detections.detach().cpu().numpy()[:, 4]
        classes = detections.detach().cpu().numpy()[:, 5]
        indexes = np.where(scores > self.prediction_confidence_threshold)[0]
        boxes = boxes[indexes]

        return {"boxes": boxes, "scores": scores[indexes], "classes": classes[indexes]}

    def __rescale_bboxes(self, predicted_bboxes, image_sizes):
        scaled_bboxes = []
        for bboxes, img_dims in zip(predicted_bboxes, image_sizes):
            im_h, im_w = img_dims

            if len(bboxes) > 0:
                scaled_bboxes.append(
                    (
                        np.array(bboxes)
                        * [
                            im_w / self.img_size,
                            im_h / self.img_size,
                            im_w / self.img_size,
                            im_h / self.img_size,
                        ]
                    ).tolist()
                )
            else:
                scaled_bboxes.append(bboxes)

        return scaled_bboxes

dm = EfficientDetDataModule(train_dataset_adaptor=electoral_train_ds,
        validation_dataset_adaptor=electoral_val_ds,
        num_workers=4,
        batch_size=4)

model = EfficientDetModel(
    num_classes=43,
    img_size=2048
    )

from pytorch_lightning import Trainer
trainer = Trainer(
         max_epochs=10, num_sanity_val_steps=1,
    )

# Commented out IPython magic to ensure Python compatibility.
# %env PYTORCH_NO_CUDA_MEMORY_CACHING=1

import gc
import torch

# # Delete large variables
# del variables

# Collect garbage
gc.collect()
torch.cuda.empty_cache()

trainer.fit(model, dm)

torch.save(model.state_dict(),'trained_effdet')